<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>图的DFS、BFS算法细节</title>
    <url>/2020/%E4%B8%B4%E7%95%8C%E7%9F%A9%E9%98%B5%E7%9A%84DFS%E3%80%81BFS%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<p>　　深度优先算法（DFS）和广度优先算法（BFS）在许多<strong>树</strong>和<strong>图</strong>的问题上都能派上用场。基于树的DFS、BFS比较简单，但基于图DFS、BFS在传参的时候有一些小细节得注意一下。不同于树，由于树具有层次结构，所以可以用一个int类型的值来表示各个节点；图则不然，需要通过（x，y）坐标的形式来表示各个节点，所以在代码编写时传递图节点（x，y）时，有哪些是值得注意的呢？</p>
<span id="more"></span>
<hr>
<p>首先，不管是树还是图，<strong>DFS</strong>都是基于<strong>递归</strong>编写的，<strong>BFS</strong>是基于<strong>队列</strong>编写的。  </p>
<ul>
<li>对于<strong>树</strong>来说，因为其<strong>具有</strong>的<strong>父与子</strong>的层次结构，DFS和BFS按上下次序遍历即可；  </li>
<li>而对于<strong>图</strong>，其结构是<strong>无上下层次</strong>的，顾在对其遍历时，我们会<strong>设置一个bool数组</strong>来记录结点是否已访问，以避免重复遍历。</li>
</ul>
<hr>
<h3 id="邻接矩阵版-DFS、BFS代码编写时图节点的传参："><a href="#邻接矩阵版-DFS、BFS代码编写时图节点的传参：" class="headerlink" title="邻接矩阵版 DFS、BFS代码编写时图节点的传参："></a><em>邻接矩阵版</em> DFS、BFS代码编写时图节点的传参：</h3><p>　　代码中有两种参数需要注意：<strong>node型的点</strong> 和 <strong>int型的点</strong>，而需注意传参的函数有<strong>bool judge()<strong>和</strong>void BFS()/DFS()<strong>。<br>　　首先两者的</strong>bool judge(int x,int y)<strong>适用传入x，y值，而对于</strong>BFS()和DFS()<strong>函数的参数则没有限制，传</strong>单int</strong>(int v)、<strong>node</strong>或者<strong>双int</strong>(int x,int y)都行，但node型结构体都得定义。  </p>
<hr>
<h3 id="如何将双int型-x-y-匹配到node型呢？"><a href="#如何将双int型-x-y-匹配到node型呢？" class="headerlink" title="如何将双int型(x,y)匹配到node型呢？"></a>如何将<strong>双int型(x,y)<strong>匹配到</strong>node型</strong>呢？</h3><p>1.对于传x,y的BFS，node只作为队列的变量，如下</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> x,<span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">truequeue&lt;node&gt; Q;</span><br><span class="line">truenode Node;</span><br><span class="line">trueNode.x=x,Node.y=y;</span><br><span class="line">trueQ.<span class="built_in">push</span>(Node);</span><br><span class="line">    flag[x][y]=<span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">while</span>(!Q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">     ...   </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里node的作用仅是用来便于存储队列里的(x,y)点；</p>
<p>​        2.对于传node的BFS，</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">(node S)</span></span>&#123;</span><br><span class="line">truequeue&lt;node&gt; Q;</span><br><span class="line">trueQ.<span class="built_in">push</span>(S);</span><br><span class="line">true<span class="keyword">while</span>(!Q.<span class="built_in">empty</span>())&#123;</span><br><span class="line">truetrue...</span><br><span class="line">true&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由此可见，<strong>queue Q里永远存的是node型</strong>，但如果直接传node型，将难以记录<strong>连通块</strong>的个数</p>
<p>总结：  </p>
<ol>
<li>对于大多数情况：bool judge的参数为(int x,int y);  </li>
<li>DFS<strong>两种都行</strong>(int x,int y)/DFS(int v),分别对应<strong>bool flag[x] [y]<strong>和</strong>bool flag[v]</strong>;  </li>
<li>BFS也是<strong>两种都行</strong>(int x,int y)/BFS(int v),如上。</li>
</ol>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>BFS</tag>
        <tag>DFS</tag>
        <tag>图</tag>
        <tag>c++</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>声明</title>
    <url>/1900/%E5%A3%B0%E6%98%8E/</url>
    <content><![CDATA[<p>本博客模板是在“班班”的博客<a href="https://github.com/lei2rock/blog"><i class="fab fa-fw fa-github"></i>Lei2rock/Blog</a>基础上进行的修改，感谢班班，原博客地址：<a href="https://blog.dlzhang.com/">https://blog.dlzhang.com</a></p>
]]></content>
  </entry>
  <entry>
    <title>NLP基础：从分词和向量化说起</title>
    <url>/2021/NLP%E5%9F%BA%E7%A1%80%EF%BC%9A%E4%BB%8E%E5%88%86%E8%AF%8D%E5%92%8C%E5%90%91%E9%87%8F%E5%8C%96%E8%AF%B4%E8%B5%B7/</url>
    <content><![CDATA[<p>　　在做NLP任务时，首先需要对文本数据进行“分词”和“向量化”处理。直观上来说，一个好的文本分词结果对模型的语义训练起着非常重要的作用。分词方式从简单的字符划分，到语言模型（LM），再到维特比算法的约束；字符向量化从基于词频、到TF-IDF、再到word2vec，这些文本处理手段使得NLP的基石越打越牢！</p>
<span id="more"></span>
<hr>
<h2 id="字符分词，词频向量化"><a href="#字符分词，词频向量化" class="headerlink" title="字符分词，词频向量化"></a>字符分词，词频向量化</h2><p>　　最简单的划分方法，首先统计整个文档的字符数，然后记录每个字符出现的次数即可。例如：</p>
<blockquote>
<p>“我是一名研究生。<br>我在杭州电子科技大学学习。<br>我的研究领域为自然语言处理。”</p>
</blockquote>
<p>　　该迷你数据集包含三条数据，共32个不同的字符，其中“我”的词频为3，“学”的词频为2，其他均为1。首先将32个字符建立成词典：  </p>
<table>
<thead>
<tr>
<th align="center">index</th>
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">…</th>
<th align="center">31</th>
</tr>
</thead>
<tbody><tr>
<td align="center">char</td>
<td align="center">我</td>
<td align="center">是</td>
<td align="center">一</td>
<td align="center">名</td>
<td align="center">…</td>
<td align="center">理</td>
</tr>
</tbody></table>
<p>基于词频的向量化：</p>
<blockquote>
<p>[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, …, 0],<br>[1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, …, 0],<br>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …, 1]]</p>
</blockquote>
<p>这就是最简单的通过词频来做文本数据的向量化，但是呢该方法存在两个主要的问题：</p>
<ol>
<li>本末倒置。一些没有什么实际含义的常用词词频会特别高，比如“我”、“是”、“。”，而某些词频低的字符可能包含着非常重要和关键的信息，比如“杭”、“研”、“语”等，这样的表示方式会让模型关注无意义的词，而忽略真正的语义信息；</li>
<li>维度爆炸，数据稀疏。当数据量大起来是，向量长度会急剧增加，而且存在大量的稀疏数据。</li>
</ol>
<hr>
<h2 id="字符分词，TF-IDF向量化"><a href="#字符分词，TF-IDF向量化" class="headerlink" title="字符分词，TF-IDF向量化"></a>字符分词，TF-IDF向量化</h2><p>　　为了解决上述问题，学者提出了TF-IDF。“TF-IDF”即“词频-逆文本频率”，它由两部分组成：TF和IDF。<br>　　前面的TF也就是我们前面说到的词频，我们之前做的向量化也就是做了文本中各个词的出现频率统计，并作为文本特征，这个很好理解。关键是后面的这个IDF，即“逆文本频率”如何理解。在上一节中，我们看到几乎所有文本都会出现的”我”词频虽然高，但是重要性却应该比词频低的”杭”和是要低的。IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。<br>　　概括来讲， IDF反应了一个词在所有文本中出现的频率，如果一个词在很多的文本中出现，那么它的IDF值应该低，比如上文中的“我”。而反过来如果一个词在比较少的文本中出现，那么它的IDF值应该高。比如一些专有的字符如“杭”。这样的词IDF值应该高。一个极端的情况，如果一个词在所有的文本中都出现，那么它的IDF值应该为0。<br>　　上面是从定性上说明的IDF的作用，那么如何对一个词的IDF进行定量分析呢？这里直接给出一个词x的IDF的基本公式如下：</p>
<p align=center>$$IDF(x)=\log \frac{N}{N(x)}$$</p>
　　其中，N代表语料库中文本的总数，而N(x)代表语料库中包含词x的文本总数。上面的IDF公式已经可以使用了，但是在一些特殊的情况会有一些小问题，比如某一个生僻词在语料库中没有，这样我们的分母为0， IDF没有意义了。所以常用的IDF我们需要做一些平滑，使语料库中没有出现的词也可以得到一个合适的IDF值。平滑的方法有很多种，最常见的IDF平滑后的公式之一为：
<p align=center>$$IDF(x)=\log \frac{N+1}{N(x)+1}+1$$</p>
　　有了IDF的定义，我们就可以计算某一个词的TF-IDF值了：
<p align=center>$$TF-IDF(x)=TF(x) * IDF(x)$$</p>
　　TF-IDF相较于词频在文本表示方面有所提升，但在中文领域，基于字符的分词方法依旧会存在问题，比如：“菜鸟”和“菜”+“鸟”的两种分词形式对模型的训练影响可能存在着完全不同的影响。
***

<h2 id="语言模型（LM）分词"><a href="#语言模型（LM）分词" class="headerlink" title="语言模型（LM）分词"></a>语言模型（LM）分词</h2><p>　　为了解决上述问题，相关学者进一步提出了语言模型的分词策略，简而言之就是每种分词片段占的权重不一样，即将“我爱机器学习”分成“我、爱、机器学习”比“我、爱、机器、学习”的概率要大一些。<br>　　再举个例子： 给定词典=[我们 学习 人工 智能 人工智能 未来 是]， 另外我们给定unigram概率：p(我们)=0.25, p(学习)=0.15, p(人工)=0.05, p(智能)=0.1, p(人工智能)=0.2, p(未来)=0.1, p(是)=0.15</p>
<h4 id="Step-1-对于给定字符串：”我们学习人工智能，人工智能是未来“-找出所有可能的分割方式："><a href="#Step-1-对于给定字符串：”我们学习人工智能，人工智能是未来“-找出所有可能的分割方式：" class="headerlink" title="Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式："></a>Step 1: 对于给定字符串：”我们学习人工智能，人工智能是未来“, 找出所有可能的分割方式：</h4><ol>
<li>[我们，学习，人工智能，人工智能，是，未来]</li>
<li>[我们，学习，人工，智能，人工智能，是，未来]</li>
<li>[我们，学习，人工，智能，人工，智能，是，未来]</li>
<li>[我们，学习，人工智能，人工，智能，是，未来]<br>…….</li>
</ol>
<h4 id="Step-2-我们也可以计算出每一个切分之后句子的概率："><a href="#Step-2-我们也可以计算出每一个切分之后句子的概率：" class="headerlink" title="Step 2: 我们也可以计算出每一个切分之后句子的概率："></a>Step 2: 我们也可以计算出每一个切分之后句子的概率：</h4><ol>
<li>p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)</li>
<li>p(我们，学习，人工，智能，人工智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工智能)-log p(是)-log p(未来)</li>
<li>p(我们，学习，人工，智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工)-log p(智能)-log p(人工)-log p(智能)-log p(是)-log p(未来)</li>
<li>p(我们，学习，人工智能，人工，智能，是，未来)=-log p(我们)-log p(学习)-log p(人工智能)-log p(人工)-log p(智能)-log(是)-log p(未来)<br>…..</li>
</ol>
<h4 id="Step-3-返回第二步中概率最大的结果："><a href="#Step-3-返回第二步中概率最大的结果：" class="headerlink" title="Step 3: 返回第二步中概率最大的结果："></a>Step 3: 返回第二步中概率最大的结果：</h4><ul>
<li><input checked="" disabled="" type="checkbox"> p(我们，学习，人工智能，人工智能，是，未来)= -log p(我们)-log p(学习)-log p(人工智能)-log p(人工智能)-log p(是)-log p(未来)</li>
</ul>
<hr>
<h2 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h2><p>　　上述方法存在一个很严重的问题：时间复杂度太高了。 为了解决这一问题，人们提出了「<a href="../%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95">维特比算法</a>」，基于动态规划来优化最大切分句子概率的查找，具体细节可查看上一篇博文。<br>　　动态规划不仅是NLP领域里用的特别多的算法，在各大面试中也是常被拿来考察的一类算法，这里列举讲解了十个非常经典的动态规划（DP）题：“<a href="../%E5%8D%81%E5%A4%A7%E7%BB%8F%E5%85%B8DP%E9%A2%98">十大经典DP题</a>”，有兴趣的同学可以了解一下。</p>
<hr>
]]></content>
      <categories>
        <category>NLP基础</category>
      </categories>
      <tags>
        <tag>分词</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>维特比算法</title>
    <url>/2021/%E7%BB%B4%E7%89%B9%E6%AF%94%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<span id="more"></span>]]></content>
      <categories>
        <category>NLP基础</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>NLP</tag>
        <tag>维特比算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习基础：极大似然估计与贝叶斯估计</title>
    <url>/2021/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%EF%BC%9A%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E4%B8%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1/</url>
    <content><![CDATA[<p>　</p>
<span id="more"></span>
<p><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_01.png"><br><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_02.png"></p>
]]></content>
      <categories>
        <category>机器学习基础</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>极大似然估计</tag>
        <tag>贝叶斯估计</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP基础：编辑距离</title>
    <url>/2021/NLP%E5%9F%BA%E7%A1%80%EF%BC%9A%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB/</url>
    <content><![CDATA[<p><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20210819150819.png"><br>　　“未闻其名，先做其题”——第一次遇到这个概念我还很“年轻”，是在Leetcode上做的一道题，当时并没有在意这个“编辑距离”，以为又是哪个“二愣子”为了算法而出的算法，也没有去想这道题如何去运用并解决实际问题的，实属惭愧~</p>
<span id="more"></span>
<hr>
<h3 id="计算两个字符串的编辑距离"><a href="#计算两个字符串的编辑距离" class="headerlink" title="计算两个字符串的编辑距离"></a>计算两个字符串的编辑距离</h3><p>编辑距离可以用来计算两个字符串的相似度，它的应用场景很多，其中之一是拼写纠正（spell correction）。<br>编辑距离的定义是给定两个字符串word1和word2，我们假定有三个不同的操作：1.插入新的字符；2.替换字符；3.删除一个字符。每一个操作的代价为1.  我们要计算通过最少多少代价cost可以把word1转换成word2. </p>
<p>举个例子：</p>
<blockquote>
输入：word1 = "geek", word2 = "gesek"<br>
输出：1<br>
解释：<br>
geek -> gesek (在 'ge' 后插入 's')<br>
</blockquote>
　
<blockquote>
输入：word1 = "horse", word2 = "ros"<br>
输出：3<br>
解释：<br>
horse -> rorse (将 'h' 替换为 'r')<br>
rorse -> rose (删除 'r')<br>
rose -> ros (删除 'e')<br>
</blockquote>  

<p>这是一道非常经典的<a href="">DP算法题</a>。以下引用leetcode的官方讲解（tajiangdebiwohao~shihua）  </p>
<p>题目给定了两个单词，设为 A 和 B，这样我们就能够六种操作方法。<br>但我们可以发现，如果我们有单词 A 和单词 B：</p>
<ul>
<li>对单词 A 删除一个字符和对单词 B 插入一个字符是等价的。例如当单词 A 为 doge，单词 B 为 dog 时，我们既可以删除单词 A 的最后一个字符 e，得到相同的 dog，也可以在单词 B 末尾添加一个字符 e，得到相同的 doge；</li>
<li>同理，对单词 B 删除一个字符和对单词 A 插入一个字符也是等价的；</li>
<li>对单词 A 替换一个字符和对单词 B 替换一个字符是等价的。例如当单词 A 为 bat，单词 B 为 cat 时，我们修改单词 A 的第一个字母 b -&gt; c，和修改单词 B 的第一个字母 c -&gt; b 是等价的。</li>
</ul>
<p>这样一来，本质不同的操作实际上只有三种：</p>
<ul>
<li>在单词 A 中插入一个字符；</li>
<li>在单词 B 中插入一个字符；</li>
<li>修改单词 A 的一个字符。</li>
</ul>
<p>这样一来，我们就可以把原问题转化为规模较小的子问题。我们用 A = horse，B = ros 作为例子，来看一看是如何把这个问题转化为规模较小的若干子问题的。</p>
<ul>
<li><strong>在单词 A 中插入一个字符</strong>：如果我们知道 horse 到 ro 的编辑距离为 a，那么显然 horse 到 ros 的编辑距离不会超过 a + 1。这是因为我们可以在 a 次操作后将 horse 和 ro 变为相同的字符串，只需要额外的 1 次操作，在单词 A 的末尾添加字符 s，就能在 a + 1 次操作后将 horse 和 ro 变为相同的字符串；</li>
<li><strong>在单词 B 中插入一个字符</strong>：如果我们知道 hors 到 ros 的编辑距离为 b，那么显然 horse 到 ros 的编辑距离不会超过 b + 1，原因同上；</li>
<li><strong>修改单词 A 的一个字符</strong>：如果我们知道 hors 到 ro 的编辑距离为 c，那么显然 horse 到 ros 的编辑距离不会超过 c + 1，原因同上。</li>
</ul>
<p>那么从 horse 变成 ros 的编辑距离应该为 min(a + 1, b + 1, c + 1)。</p>
<p><strong>注意</strong>：为什么我们总是在单词 A 和 B 的末尾插入或者修改字符，能不能在其它的地方进行操作呢？答案是可以的，但是我们知道，操作的顺序是不影响最终的结果的。例如对于单词 cat，我们希望在 c 和 a 之间添加字符 d 并且将字符 t 修改为字符 b，那么这两个操作无论为什么顺序，都会得到最终的结果 cdab。</p>
<p>你可能觉得 horse 到 ro 这个问题也很难解决。但是没关系，我们可以继续用上面的方法拆分这个问题，对于这个问题拆分出来的所有子问题，我们也可以继续拆分，直到：</p>
<ul>
<li>字符串 A 为空，如从 转换到 ro，显然编辑距离为字符串 B 的长度，这里是 2；</li>
<li>字符串 B 为空，如从 horse 转换到 ，显然编辑距离为字符串 A 的长度，这里是 5。</li>
</ul>
<p>因此，我们就可以使用动态规划来解决这个问题了。我们用 D[i][j] 表示 A 的前 i 个字母和 B 的前 j 个字母之间的编辑距离。</p>
<p>当我们获得 D[i][j-1]，D[i-1][j] 和 D[i-1][j-1] 的值之后就可以计算出 D[i][j]。</p>
<ul>
<li>D[i][j-1] 为 A 的前 i 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们在 A 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i][j-1] + 1；</li>
<li>D[i-1][j] 为 A 的前 i - 1 个字符和 B 的前 j 个字符编辑距离的子问题。即对于 A 的第 i 个字符，我们在 B 的末尾添加了一个相同的字符，那么 D[i][j] 最小可以为 D[i-1][j] + 1；</li>
<li>D[i-1][j-1] 为 A 前 i - 1 个字符和 B 的前 j - 1 个字符编辑距离的子问题。即对于 B 的第 j 个字符，我们修改 A 的第 i 个字符使它们相同，那么 D[i][j] 最小可以为 D[i-1][j-1] + 1。特别地，如果 A 的第 i 个字符和 B 的第 j 个字符原本就相同，那么我们实际上不需要进行修改操作。在这种情况下，D[i][j] 最小可以为 D[i-1][j-1]。</li>
</ul>
<p>那么我们可以写出如下的状态转移方程：</p>
<ul>
<li>若 A 和 B 的最后一个字母相同：<br>\begin{aligned}<br>D[i][j] &amp;=\min (D[i][j-1]+1, D[i-1][j]+1, D[i-1][j-1])<br>\end{aligned}<br>\begin{aligned}<br>=1+\min (D[i][j-1], D[i-1][j], D[i-1][j-1]-1)<br>\end{aligned}</li>
</ul>
<ul>
<li>若 A 和 B 的最后一个字母不同：<br>$$<br>D[i][j]=1+\min (D[i][j-1], D[i-1][j], D[i-1][j-1])<br>$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minDistance</span>(<span class="params">word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span></span><br><span class="line">    n = <span class="built_in">len</span>(word1)</span><br><span class="line">    m = <span class="built_in">len</span>(word2)</span><br><span class="line">    <span class="comment"># 有一个字符串为空串</span></span><br><span class="line">    <span class="keyword">if</span> n * m == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> n + m</span><br><span class="line">    <span class="comment"># DP 数组</span></span><br><span class="line">    D = [ [<span class="number">0</span>] * (m + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># 边界状态初始化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>):</span><br><span class="line">        D[i][<span class="number">0</span>] = i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>):</span><br><span class="line">        D[<span class="number">0</span>][j] = j</span><br><span class="line">    <span class="comment"># 计算所有 DP 值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">            left = D[i - <span class="number">1</span>][j] + <span class="number">1</span></span><br><span class="line">            down = D[i][j - <span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">            left_down = D[i - <span class="number">1</span>][j - <span class="number">1</span>] </span><br><span class="line">            <span class="keyword">if</span> word1[i - <span class="number">1</span>] != word2[j - <span class="number">1</span>]:</span><br><span class="line">                left_down += <span class="number">1</span></span><br><span class="line">            D[i][j] = <span class="built_in">min</span>(left, down, left_down) </span><br><span class="line">    <span class="keyword">return</span> D[n][m]</span><br></pre></td></tr></table></figure>

<p><strong>复杂度分析</strong></p>
<ul>
<li>时间复杂度 ：O(mn)O(mn)，其中 mm 为 word1 的长度，nn 为 word2 的长度。</li>
<li>空间复杂度 ：O(mn)O(mn)，我们需要大小为 O(mn)O(mn) 的 DD 数组来记录状态值。</li>
</ul>
<h3 id="生成指定编辑距离的单词"><a href="#生成指定编辑距离的单词" class="headerlink" title="生成指定编辑距离的单词"></a>生成指定编辑距离的单词</h3><p>给定一个单词，我们也可以生成编辑距离为K的单词列表。 比如给定 str=”apple”，K=1, 可以生成“appl”, “appla”, “pple”…等<br>下面看怎么生成这些单词。 还是用英文的例子来说明。 仍然假设有三种操作 - 插入，删除，替换</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_edit_one</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    给定一个字符串，生成编辑距离为1的字符串列表。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    letters    = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">    splits = [(<span class="built_in">str</span>[:i], <span class="built_in">str</span>[i:])<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="built_in">str</span>)+<span class="number">1</span>)]</span><br><span class="line">    inserts = [L + c + R <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    deletes = [L + R[<span class="number">1</span>:] <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R]</span><br><span class="line">    replaces = [L + c + R[<span class="number">1</span>:] <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">if</span> R <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#return set(splits)</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">set</span>(inserts + deletes + replaces)</span><br><span class="line">edit_one = generate_edit_one(<span class="string">&quot;apple&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">len</span>(edit_one), edit_one)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_edit_two</span>(<span class="params"><span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    给定一个字符串，生成编辑距离不大于2的字符串</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> [e2 <span class="keyword">for</span> e1 <span class="keyword">in</span> generate_edit_one(<span class="built_in">str</span>) <span class="keyword">for</span> e2 <span class="keyword">in</span> generate_edit_one(e1)]</span><br><span class="line">edit_two = generate_edit_two(<span class="string">&quot;apple&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">len</span>(edit_two), edit_two)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP基础</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>NLP</tag>
        <tag>编辑距离</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP任务示例：文本自动纠错（基于统计模型）</title>
    <url>/2021/NLP%E4%BB%BB%E5%8A%A1%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%96%87%E6%9C%AC%E8%87%AA%E5%8A%A8%E7%BA%A0%E9%94%99%EF%BC%88%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B%EF%BC%89/</url>
    <content><![CDATA[<p><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20210819134228.png"><br>　　其实文本纠错的原理和我们直觉认为上面单词写错了是一样的。我们之所以认为上面的 <em>appl</em> 是错误的，是因为我们拥有大量的 <em>apple</em>和<em>I have an apple.</em> 的先验知识，<em>appl</em> 是啥？不知道。那机器让如何拥有这种知识呢，答案是：<strong>统计学习</strong>和<strong>神经网络</strong>。我们先来看下<strong>基于统计模型的文本自动纠错</strong>~</p>
<span id="more"></span>
<hr>
<p>整个文本纠错任务可以分为以下几个步骤：</p>
<ol>
<li>首先第一步：找到出错的单词；</li>
<li>然后用<a href="../NLP%E5%9F%BA%E7%A1%80%EF%BC%9A%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB">编辑距离</a>求出候选的正确的单词集合；</li>
<li>基于统计方法训练语言模型；</li>
<li>通过贝叶斯定理和语言模型找出最符合文本上下文的词。</li>
</ol>
<hr>
<h3 id="第一步：找出出错的单词"><a href="#第一步：找出出错的单词" class="headerlink" title="第一步：找出出错的单词"></a>第一步：找出出错的单词</h3><p>比如“I have an apply”，通过大量的先验知识，我们可以很直观的知道这里将“apple”错写成了“apply”，那么机器如何知道这一点呢？这就需要靠<a href="">语言模型（LM）</a>了，我们可以设定一个阈值，当bi-gram LM（一般是）输出的概率小于该阈值时就认为该单词为写错的单词，比如这里的P(apply|an)应该会低于阈值。这里为了简单演示纠错整个原理，将这一步简化成不在词典里的单词。</p>
<h3 id="第二步：用编辑距离求出候选的正确的单词集合"><a href="#第二步：用编辑距离求出候选的正确的单词集合" class="headerlink" title="第二步：用编辑距离求出候选的正确的单词集合"></a>第二步：用<a href="../NLP%E5%9F%BA%E7%A1%80%EF%BC%9A%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB">编辑距离</a>求出候选的正确的单词集合</h3><p>这里编辑距离选1，2都行，2基本上已经涵盖了99%的错词情况。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_candidates</span>(<span class="params">word</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    word: 给定的输入（错误的输入） </span></span><br><span class="line"><span class="string">    返回所有(valid)候选集合</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 生成编辑距离为1的单词</span></span><br><span class="line">    <span class="comment"># 1.insert 2. delete 3. replace</span></span><br><span class="line">    <span class="comment"># appl: replace: bppl, cppl, aapl, abpl... </span></span><br><span class="line">    <span class="comment">#       insert: bappl, cappl, abppl, acppl....</span></span><br><span class="line">    <span class="comment">#       delete: ppl, apl, app</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 假设使用26个字符</span></span><br><span class="line">    letters = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span> </span><br><span class="line">    </span><br><span class="line">    splits = [(word[:i], word[i:]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(word)+<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># insert操作</span></span><br><span class="line">    inserts = [L+c+R <span class="keyword">for</span> L, R <span class="keyword">in</span> splits <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    <span class="comment"># delete操作</span></span><br><span class="line">    deletes = [L+R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> splits <span class="keyword">if</span> R]</span><br><span class="line">    <span class="comment"># replace操作</span></span><br><span class="line">    replaces = [L+c+R[<span class="number">1</span>:] <span class="keyword">for</span> L,R <span class="keyword">in</span> splits <span class="keyword">if</span> R <span class="keyword">for</span> c <span class="keyword">in</span> letters]</span><br><span class="line">    </span><br><span class="line">    candidates = <span class="built_in">set</span>(inserts+deletes+replaces)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 过滤掉不存在于词典库里面的单词</span></span><br><span class="line">    <span class="keyword">return</span> [word <span class="keyword">for</span> word <span class="keyword">in</span> candidates <span class="keyword">if</span> word <span class="keyword">in</span> vocab]</span><br><span class="line"></span><br><span class="line">generate_candidates(<span class="string">&quot;appl&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="第三步：基于统计方法训练语言模型"><a href="#第三步：基于统计方法训练语言模型" class="headerlink" title="第三步：基于统计方法训练语言模型"></a>第三步：基于统计方法训练语言模型</h3><h4 id="3-1首先读取语料库（训练集）"><a href="#3-1首先读取语料库（训练集）" class="headerlink" title="3.1首先读取语料库（训练集）"></a>3.1首先读取语料库（训练集）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> reuters</span><br><span class="line">categories = reuters.categories()</span><br><span class="line">corpus = reuters.sents(categories=categories)</span><br></pre></td></tr></table></figure>
<h4 id="3-2构建语言模型-bigram（sklearn里面也有现成的包）"><a href="#3-2构建语言模型-bigram（sklearn里面也有现成的包）" class="headerlink" title="3.2构建语言模型: bigram（sklearn里面也有现成的包）"></a>3.2构建语言模型: bigram（sklearn里面也有现成的包）</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">term_count = &#123;&#125;</span><br><span class="line">bigram_count = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> corpus:</span><br><span class="line">    doc = [<span class="string">&#x27;&lt;s&gt;&#x27;</span>] + doc</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(doc)-<span class="number">1</span>):</span><br><span class="line">        <span class="comment"># bigram: [i,i+1]</span></span><br><span class="line">        term = doc[i]</span><br><span class="line">        bigram = doc[i:i+<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> term <span class="keyword">in</span> term_count:</span><br><span class="line">            term_count[term]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            term_count[term]=<span class="number">1</span></span><br><span class="line">        bigram = <span class="string">&#x27; &#x27;</span>.join(bigram)</span><br><span class="line">        <span class="keyword">if</span> bigram <span class="keyword">in</span> bigram_count:</span><br><span class="line">            bigram_count[bigram]+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            bigram_count[bigram]=<span class="number">1</span></span><br></pre></td></tr></table></figure>
<h4 id="3-3计算用户打错的概率统计-channel-probability"><a href="#3-3计算用户打错的概率统计-channel-probability" class="headerlink" title="3.3计算用户打错的概率统计 - channel probability"></a>3.3计算用户打错的概率统计 - channel probability</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">channel_prob = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> <span class="built_in">open</span>(<span class="string">&#x27;spell-errors.txt&#x27;</span>):</span><br><span class="line">    items = line.split(<span class="string">&quot;:&quot;</span>)</span><br><span class="line">    correct = items[<span class="number">0</span>].strip()</span><br><span class="line">    mistakes = [item.strip() <span class="keyword">for</span> item <span class="keyword">in</span> items[<span class="number">1</span>].strip().split(<span class="string">&quot;,&quot;</span>)]</span><br><span class="line">    channel_prob[correct] = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> mis <span class="keyword">in</span> mistakes:</span><br><span class="line">        channel_prob[correct][mis] = <span class="number">1.0</span>/<span class="built_in">len</span>(mistakes)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(channel_prob)   </span><br></pre></td></tr></table></figure>

<h3 id="第四步：通过贝叶斯定理和语言模型找出最符合文本上下文的词"><a href="#第四步：通过贝叶斯定理和语言模型找出最符合文本上下文的词" class="headerlink" title="第四步：通过贝叶斯定理和语言模型找出最符合文本上下文的词"></a>第四步：通过贝叶斯定理和语言模型找出最符合文本上下文的词</h3><p><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20210819230651.png"><br>测试集文档由sentence组成，每个sentence有一个不在词典里的错别词，找出并纠正该错别词。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">V = <span class="built_in">len</span>(term_count.keys())</span><br><span class="line"></span><br><span class="line">file = <span class="built_in">open</span>(<span class="string">&quot;testdata.txt&quot;</span>, <span class="string">&#x27;r&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">    items = line.rstrip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">    line = items[<span class="number">2</span>].split()</span><br><span class="line">    <span class="comment"># line = [&quot;I&quot;, &quot;like&quot;, &quot;playing&quot;]</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> line:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">            <span class="comment"># 需要替换word成正确的单词</span></span><br><span class="line">            <span class="comment"># Step1: 生成所有的(valid)候选集合</span></span><br><span class="line">            candidates = generate_candidates(word)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 一种方式： if candidate = [], 多生成几个candidates, 比如生成编辑距离不大于2的</span></span><br><span class="line">            <span class="comment"># TODO ： 根据条件生成更多的候选集合</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(candidates) &lt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span>   <span class="comment"># 不建议这么做（这是不对的） </span></span><br><span class="line">            probs = []</span><br><span class="line">            <span class="comment"># 对于每一个candidate, 计算它的score</span></span><br><span class="line">            <span class="comment"># score = p(correct)*p(mistake|correct)</span></span><br><span class="line">            <span class="comment">#       = log p(correct) + log p(mistake|correct)</span></span><br><span class="line">            <span class="comment"># 返回score最大的candidate</span></span><br><span class="line">            <span class="keyword">for</span> candi <span class="keyword">in</span> candidates:</span><br><span class="line">                prob = <span class="number">0</span></span><br><span class="line">                <span class="comment"># a. 计算channel probability</span></span><br><span class="line">                <span class="keyword">if</span> candi <span class="keyword">in</span> channel_prob <span class="keyword">and</span> word <span class="keyword">in</span> channel_prob[candi]:</span><br><span class="line">                    prob += np.log(channel_prob[candi][word])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    prob += np.log(<span class="number">0.0001</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># b. 计算语言模型的概率</span></span><br><span class="line">                idx = items[<span class="number">2</span>].index(word)+<span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> items[<span class="number">2</span>][idx - <span class="number">1</span>] <span class="keyword">in</span> bigram_count <span class="keyword">and</span> candi <span class="keyword">in</span> bigram_count[items[<span class="number">2</span>][idx - <span class="number">1</span>]]:</span><br><span class="line">                    prob += np.log((bigram_count[items[<span class="number">2</span>][idx - <span class="number">1</span>]][candi] + <span class="number">1.0</span>) / (</span><br><span class="line">                            term_count[bigram_count[items[<span class="number">2</span>][idx - <span class="number">1</span>]]] + V))</span><br><span class="line">                <span class="comment"># <span class="doctag">TODO:</span> 也要考虑当前 [word, post_word]</span></span><br><span class="line">                <span class="comment">#   prob += np.log(bigram概率)</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    prob += np.log(<span class="number">1.0</span> / V)</span><br><span class="line"></span><br><span class="line">                probs.append(prob)</span><br><span class="line">                </span><br><span class="line">            max_idx = probs.index(<span class="built_in">max</span>(probs))</span><br><span class="line">            <span class="built_in">print</span> (word, candidates[max_idx])</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>NLP进阶</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>文本纠错</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP任务示例：词性标注</title>
    <url>/2021/NLP%E4%BB%BB%E5%8A%A1%E7%A4%BA%E4%BE%8B%EF%BC%9A%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/</url>
    <content><![CDATA[<span id="more"></span>
<hr>
]]></content>
      <categories>
        <category>NLP任务示例</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>词性标注</tag>
      </tags>
  </entry>
  <entry>
    <title>TripPy：A_Triple_Copy_Strategy_for_Value_Independent_Neural_Dialog_State_Tracking</title>
    <url>/2021/TripPy%EF%BC%9AA-Triple-Copy-Strategy-for-Value-Independent-Neural-Dialog-State-Tracking/</url>
    <content><![CDATA[<p><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20211012151220.png"></p>
<span id="more"></span>
<hr>
<p><strong>现有的缺陷</strong><br>Slot的类型多种多样，单一的Slot Value填充方法无法满足需求。  </p>
<p><strong>本文贡献</strong><br>采用三种不同的Slot Value填充方法来完成DST任务  </p>
<ol>
<li>span prediction：从用户输入中提取候选值；  </li>
<li>copied from a system inform memory：由系统提供，然后用户确认。比如，系统：“xx餐厅的青椒炒肉挺不错的”，用户：“好，就去那吃吧”，则[餐厅-名称]为xx餐厅；</li>
<li>值共享：不同的slot有相同的value，比如[酒店-地址]和[出租车-目的地]很大程度上会形成值共享。  </li>
</ol>
<p><strong>模型架构</strong><br><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20211012161052.png"></p>
<p>模型有以下几个点得注意一下：</p>
<ol>
<li>首先模型的输入为整个对话历史；</li>
<li>对于每一个Slot都会有一个分类器（5分类和boolslot4分类）；</li>
<li>span-based value prediction模块没啥好说的了，end &lt; start时置为空；</li>
<li>System Inform Memory for Value Prediction;</li>
<li>DS Memory for Coreference Resolution: N+1(none)分类，看该slot有没有共享值；</li>
<li>Auxiliary Features</li>
</ol>
<p><strong>实验结果</strong><br>MultWoz2.1：<br><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20211012183957.png">  </p>
<p>消融实验：<br><img data-src="https://cdn.jsdelivr.net/gh/cyberfish1120/cdn/img/20211012184433.png">  </p>
<p><strong>实验总结</strong><br>模型的计算方法是DS-DST的一种改良版本吧。在特征提取这块。DS-DST 将 CLS，一个域槽对和对话上下文拼接起来。由于所有域槽对的词向量是不同的，则可以凭此遍历所有的域槽对，使得每次捕获到的特征都是根据域槽对的变化而变化。所以当使用 CLS 进行 slot gate 分类时，可以确定该 slot gate 是基于某一域槽对的，并且 CLS 表征总是不同的。但是这样的做法计算起来特别麻烦，因为如果想要向量化，必须复制 N 份上下文（N 为域槽对数量）。<br>TripPy略微地改进了它，它移除了输入中的域槽对，然后，TripPy 为每一个域槽对都设计了一个线性层用于计算 slot gate。这也能使得 CLS 的表征总是不同，因为线性层中的权重矩阵是不同的。</p>
]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>DST</tag>
      </tags>
  </entry>
</search>
